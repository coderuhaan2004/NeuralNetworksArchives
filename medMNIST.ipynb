{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlAB56_aA0ub"
      },
      "outputs": [],
      "source": [
        "!pip install medmnist pillow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "from torch.utils.data import IterableDataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import medmnist\n",
        "from medmnist import INFO, Evaluator\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqbKQnqMBCwC",
        "outputId": "08c8f213-8c81-4c9e-abae-fbe62cc06567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_flag = 'pathmnist'\n",
        "# data_flag = 'breastmnist'\n",
        "download = True\n",
        "\n",
        "NUM_EPOCHS = [2,5,10]\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "n_channels = info['n_channels']\n",
        "n_classes = len(info['label'])\n",
        "\n",
        "DataClass = getattr(medmnist, info['python_class'])"
      ],
      "metadata": {
        "id": "rEurJd0mBHyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the .npz file\n",
        "npz_file_path = '/root/.medmnist/pathmnist_224.npz'\n",
        "\n",
        "# Load the .npz file\n",
        "data = np.load(npz_file_path, mmap_mode='r',allow_pickle=False)\n",
        "\n",
        "# Check the keys in the .npz file\n",
        "print(\"Keys in the .npz file:\", data.files)\n",
        "\n",
        "# Iterate through keys and save each key as a separate .npy file\n",
        "for key in data.files:\n",
        "    print(f\"Processing and saving key: {key}\")\n",
        "\n",
        "    # Save the data for this key directly to disk\n",
        "    with open(f\"{key}.npy\", 'wb') as f:\n",
        "        np.save(f, data[key])\n",
        "\n",
        "    print(f\"Key '{key}' saved successfully.\")\n",
        "\n",
        "# Close the .npz file after processing\n",
        "data.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3rbX9zVIoHd",
        "outputId": "df6e616f-d1fa-4acf-dff1-f7d6964d5150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in the .npz file: ['train_images', 'train_labels', 'val_images', 'val_labels', 'test_images', 'test_labels']\n",
            "Processing and saving key: train_images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LazyDataset(IterableDataset):\n",
        "    def __init__(self, dataset, transform=None):\n",
        "        self.dataset = dataset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __iter__(self):\n",
        "        for idx in range(len(self.dataset)):\n",
        "            img = self.dataset[idx]  # Lazy loading of the image\n",
        "            if self.transform:\n",
        "                img = self.transform(img)  # Apply transformation\n",
        "            yield img\n",
        "\n",
        "# Transformation\n",
        "transform_data = transforms.Compose([\n",
        "    transforms.ToTensor()  # Converts image to (C, H, W) with values in [0, 1]\n",
        "])"
      ],
      "metadata": {
        "id": "ETGsW3tZBHCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load .npz file\n",
        "data = np.load('/root/.medmnist/pathmnist_224.npz')\n",
        "# Save individual arrays as .npy files for lazy loading\n",
        "np.save('data.npy', data['data'])\n",
        "np.save('labels.npy', data['labels'])\n",
        "\n",
        "# Use memory mapping for individual arrays\n",
        "data_mmap = np.load('data.npy', mmap_mode='r')\n",
        "labels_mmap = np.load('labels.npy', mmap_mode='r')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "3hWsjFhBF4Jk",
        "outputId": "35b88eae-435f-4ec7-f338-3748ee548986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'data is not a file in the archive'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-c76018cd3029>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/root/.medmnist/pathmnist_224.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Save individual arrays as .npy files for lazy loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels.npy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{key} is not a file in the archive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'data is not a file in the archive'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/root/.medmnist/pathmnist_224.npz"
      ],
      "metadata": {
        "id": "2XZDz8GKFFml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and test datasets\n",
        "train_dataset = DataClass(split='train', transform=None, download=download,size=224, mmap_mode='r')\n",
        "test_dataset = DataClass(split='test', transform=None, download=download, size=224, mmap_mode='r')\n",
        "\n",
        "# Wrap datasets with LazyDataset for lazy loading and apply transformations\n",
        "lazy_train_dataset = LazyDataset(train_dataset, transform=transform_data)\n",
        "lazy_test_dataset = LazyDataset(test_dataset, transform=transform_data)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(lazy_train_dataset, batch_size=64, pin_memory=True, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(lazy_test_dataset, batch_size=64, pin_memory=True, shuffle=False, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQKZjpmeEtm1",
        "outputId": "25990a92-2d2d-4e7c-c39b-a956ec65470b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/pathmnist_224.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_mean_std(loader):\n",
        "    mean = 0.0\n",
        "    std = 0.0\n",
        "    total_images = 0\n",
        "\n",
        "    for images, _ in tqdm(loader):\n",
        "        images = images.view(images.size(0), images.size(1), -1)  # Flatten (C, H, W) to (C, N)\n",
        "        mean += images.mean(2).sum(0)\n",
        "        std += images.std(2).sum(0)\n",
        "        total_images += images.size(0)\n",
        "\n",
        "    mean /= total_images\n",
        "    std /= total_images\n",
        "    return mean, std\n",
        "\n",
        "transform_data = transforms.Compose([\n",
        "    transforms.ToTensor()  # Converts image to (C, H, W) with values in [0, 1]\n",
        "])\n",
        "\n",
        "# Load the PathMNIST dataset\n",
        "train_dataset = DataClass(split='train', transform=transform_data, download=download,size=224, mmap_mode='r')\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, pin_memory=True,shuffle=False, num_workers=2)\n",
        "\n",
        "test_dataset = DataClass(split='test', transform=transform_data, download=download, size=224, mmap_mode='r')\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "mean_tr, std_tr = compute_mean_std(train_loader)\n",
        "mean_test, std_test = compute_mean_std(test_loader)\n",
        "\n",
        "print(\"Dataset Training Mean:\", mean_tr)\n",
        "print(\"Dataset Training Std:\", std_tr)\n",
        "print(\"Dataset Test Mean:\", mean_test)\n",
        "print(\"Dataset Test Std:\", std_test)"
      ],
      "metadata": {
        "id": "2kZNugUMpODs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fff7d849-c223-451a-ccec-6cb78681f3b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/pathmnist_224.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute PCA noise\n",
        "def add_pca_noise(img, alpha_std=0.1):\n",
        "    \"\"\"\n",
        "    Add PCA noise to an image.\n",
        "    Args:\n",
        "        img: A PIL Image or Tensor (C x H x W).\n",
        "        alpha_std: The standard deviation of the noise to add.\n",
        "    Returns:\n",
        "        A Tensor with PCA noise added.\n",
        "    \"\"\"\n",
        "    if isinstance(img, Image.Image):  # If the input is a PIL Image\n",
        "        img = transforms.ToTensor()(img)  # Convert to Tensor (C, H, W)\n",
        "\n",
        "    img_np = img.permute(1, 2, 0).numpy()  # Convert to NumPy (H, W, C)\n",
        "    img_flat = img_np.reshape(-1, 3)  # Reshape to (N, 3) where N = H*W\n",
        "\n",
        "    # Compute mean and covariance\n",
        "    mean = np.mean(img_flat, axis=0)\n",
        "    centered = img_flat - mean\n",
        "    cov = np.cov(centered, rowvar=False)\n",
        "\n",
        "    # Eigen decomposition\n",
        "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
        "\n",
        "    # Add noise along the principal components\n",
        "    noise = np.random.normal(0, alpha_std, size=3) * eigvals\n",
        "    noise = eigvecs @ noise  # Project noise into RGB space\n",
        "\n",
        "    img_flat += noise  # Add noise to each pixel\n",
        "    img_flat = np.clip(img_flat, 0, 1)  # Clip values to valid range\n",
        "    img_noisy = img_flat.reshape(img_np.shape)  # Reshape back to original\n",
        "\n",
        "    return torch.tensor(img_noisy).permute(2, 0, 1)  # Convert back to Tensor (C, H, W)\n",
        "\n",
        "# preprocessing\n",
        "train_data_transform = transforms.Compose([\n",
        "    # Random crop and resize\n",
        "    transforms.RandomResizedCrop(size=224, scale=(0.08, 1.0), ratio=(3/4, 4/3)),\n",
        "\n",
        "    # Random horizontal flip\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "\n",
        "    # Color jitter\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "\n",
        "    # PCA noise\n",
        "    transforms.Lambda(lambda img: add_pca_noise(img, alpha_std=0.1)),\n",
        "\n",
        "    # Normalize\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[mean_tr[0].item(),mean_tr[1].item(),mean_tr[2].item()], std=[std_tr[0].item(), std_tr[1].item(), std_tr[2].item()])\n",
        "])\n",
        "\n",
        "# Define test dataset transformations using calculated mean and std\n",
        "test_data_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[mean_test[0].item(), mean_test[1].item(), mean_test[2].item()],\n",
        "                         std=[std_test[0].item(), std_test[1].item(), std_test[2].item()])\n",
        "])\n",
        "\n",
        "# load the data\n",
        "train_dataset = DataClass(split='train', transform=train_data_transform, download=download, size=224, mmap_mode='r')\n",
        "test_dataset = DataClass(split='test', transform=test_data_transform, download=download, size=224, mmap_mode='r')\n",
        "\n",
        "#pil_dataset = DataClass(split='train', download=download)\n",
        "\n",
        "# Define the sizes for train and validation splits\n",
        "#train_size = len(train_dataset)  # 80% of the data\n",
        "#val_size = len(train_dataset) - train_size  # Remaining 20%\n",
        "\n",
        "# Split the train_dataset into training and validation sets\n",
        "#train_subset, val_subset = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders for training and validation sets\n",
        "train_loader = data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "#val_loader = data.DataLoader(dataset=val_subset, batch_size=2*BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Test DataLoader remains unchanged\n",
        "test_loader = data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6zj0hQTBN8R",
        "outputId": "7f1bc45f-5f44-4e88-ade9-cc6f371d164e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n",
            "Using downloaded and verified file: /root/.medmnist/pathmnist.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset)\n",
        "print(\"===================\")\n",
        "print(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0u3bo9cBQS2",
        "outputId": "ba54c526-42a7-40ce-d4e6-132b1d83c0ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset PathMNIST of size 28 (pathmnist)\n",
            "    Number of datapoints: 89996\n",
            "    Root location: /root/.medmnist\n",
            "    Split: train\n",
            "    Task: multi-class\n",
            "    Number of channels: 3\n",
            "    Meaning of labels: {'0': 'adipose', '1': 'background', '2': 'debris', '3': 'lymphocytes', '4': 'mucus', '5': 'smooth muscle', '6': 'normal colon mucosa', '7': 'cancer-associated stroma', '8': 'colorectal adenocarcinoma epithelium'}\n",
            "    Number of samples: {'train': 89996, 'val': 10004, 'test': 7180}\n",
            "    Description: The PathMNIST is based on a prior study for predicting survival from colorectal cancer histology slides, providing a dataset (NCT-CRC-HE-100K) of 100,000 non-overlapping image patches from hematoxylin & eosin stained histological images, and a test dataset (CRC-VAL-HE-7K) of 7,180 image patches from a different clinical center. The dataset is comprised of 9 types of tissues, resulting in a multi-class classification task. We resize the source images of 3×224×224 into 3×28×28, and split NCT-CRC-HE-100K into training and validation set with a ratio of 9:1. The CRC-VAL-HE-7K is treated as the test set.\n",
            "    License: CC BY 4.0\n",
            "===================\n",
            "Dataset PathMNIST of size 28 (pathmnist)\n",
            "    Number of datapoints: 7180\n",
            "    Root location: /root/.medmnist\n",
            "    Split: test\n",
            "    Task: multi-class\n",
            "    Number of channels: 3\n",
            "    Meaning of labels: {'0': 'adipose', '1': 'background', '2': 'debris', '3': 'lymphocytes', '4': 'mucus', '5': 'smooth muscle', '6': 'normal colon mucosa', '7': 'cancer-associated stroma', '8': 'colorectal adenocarcinoma epithelium'}\n",
            "    Number of samples: {'train': 89996, 'val': 10004, 'test': 7180}\n",
            "    Description: The PathMNIST is based on a prior study for predicting survival from colorectal cancer histology slides, providing a dataset (NCT-CRC-HE-100K) of 100,000 non-overlapping image patches from hematoxylin & eosin stained histological images, and a test dataset (CRC-VAL-HE-7K) of 7,180 image patches from a different clinical center. The dataset is comprised of 9 types of tissues, resulting in a multi-class classification task. We resize the source images of 3×224×224 into 3×28×28, and split NCT-CRC-HE-100K into training and validation set with a ratio of 9:1. The CRC-VAL-HE-7K is treated as the test set.\n",
            "    License: CC BY 4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(Xtr[0])[0].shape #1st image of train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etLmCOUkBY6g",
        "outputId": "a629655b-5ab2-45a9-e430-881a05d9335c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"Basic Residual Block.\"\"\"\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += identity\n",
        "        out = F.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "oXYbBwhyBu_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.init as init\n",
        "class ResNet(nn.Module):\n",
        "    \"\"\"ResNet Architecture.\"\"\"\n",
        "    def __init__(self, block, layers, num_classes):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "\n",
        "        # Input Stem\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Residual Layers\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "\n",
        "        # Final Output Layer\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, out_channels * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.in_channels, out_channels))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "PHItUZGqBxEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_weights(module):\n",
        "    \"\"\"\n",
        "    Initialize weights of the module according to the Xavier algorithm.\n",
        "    \"\"\"\n",
        "    if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "        # Xavier initialization\n",
        "        fan_in = module.weight.size(1)\n",
        "        fan_out = module.weight.size(0)\n",
        "        a = math.sqrt(6 / (fan_in + fan_out))\n",
        "        nn.init.uniform_(module.weight, -a, a)\n",
        "        if module.bias is not None:\n",
        "            nn.init.constant_(module.bias, 0)\n",
        "    elif isinstance(module, nn.BatchNorm2d):\n",
        "        # BatchNorm initialization\n",
        "        nn.init.constant_(module.weight, 1)  # gamma = 1\n",
        "        nn.init.constant_(module.bias, 0)    # beta = 0"
      ],
      "metadata": {
        "id": "aNW0olOgTUXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet(ResidualBlock, [3,4,6,3] ,num_classes=n_classes).to(device)\n",
        "model.apply(initialize_weights) #apply the initializer to the entire model\n",
        "# define loss function and optimizer\n",
        "if task == \"multi-label, binary-class\":\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "else:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#Define initial learning rate and batch size\n",
        "initial_lr = 0.1\n",
        "base_batch_size = 256\n",
        "current_batch_size = BATCH_SIZE\n",
        "\n",
        "scaled_lr = initial_lr * (current_batch_size / base_batch_size)\n",
        "# Define optimizer with the scaled learning rate\n",
        "optimizer = optim.SGD(model.parameters(), lr=scaled_lr, momentum=0.9, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "L3AIGmuhzq1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Simple CNN"
      ],
      "metadata": {
        "id": "Ibp-iFr6d00R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a simple CNN model\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 16, kernel_size=3),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(p=0.3) #Change\n",
        "            )\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, kernel_size=3),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(16, 64, kernel_size=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU())\n",
        "\n",
        "        self.layer5 = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(64 * 5 * 5, 128),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(p=0.5), #Change\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(p=0.5),  #Change\n",
        "            nn.Linear(128, num_classes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "model = Net(in_channels=n_channels, num_classes=n_classes).to(device)\n",
        "#model.apply(initialize_weights)\n",
        "\n",
        "# define loss function and optimizer\n",
        "if task == \"multi-label, binary-class\":\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "else:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9,weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "_7f1RM7Kd4JT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet(ResidualBlock, [3,4,6,3] ,in_channels=n_channels, num_classes=n_classes).to(device)\n",
        "# define loss function and optimizer\n",
        "if task == \"multi-label, binary-class\":\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "else:\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay = 1e-4)"
      ],
      "metadata": {
        "id": "_-BLRmSFB9Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(NUM_EPOCHS[2]):\n",
        "    train_correct = 0\n",
        "    train_total = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    for inputs, targets in tqdm(train_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward + Backward + Optimize\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        if task == 'multi-label, binary-class':\n",
        "            targets = targets.to(torch.float32)\n",
        "            loss = criterion(outputs, targets)\n",
        "        else:\n",
        "            targets = targets.squeeze().long()\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_loss = total_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Training Loss: {train_loss:.4f}')"
      ],
      "metadata": {
        "id": "TyZ7wZlgCXlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation Phase\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in tqdm(val_loader):\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            if task == 'multi-label, binary-class':\n",
        "                targets = targets.to(torch.float32)\n",
        "                loss = criterion(outputs, targets)\n",
        "            else:\n",
        "                targets = targets.squeeze().long()\n",
        "                loss = criterion(outputs, targets)\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += targets.size(0)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "\n",
        "    val_loss = total_val_loss / len(val_loader)\n",
        "    val_accuracy = 100 * correct / total\n",
        "    print(f'Epoch [{epoch+1}/{NUM_EPOCHS}], Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "o3XY3j-PWzLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "val_loss = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in tqdm(test_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        if task == 'multi-label, binary-class':\n",
        "            targets = targets.to(torch.float32)\n",
        "            loss_test = criterion(outputs, targets)\n",
        "        else:\n",
        "            targets = targets.squeeze().long()\n",
        "            loss_test = criterion(outputs, targets)\n",
        "        val_loss += loss_test.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += targets.size(0)\n",
        "        correct += (predicted == targets).sum().item()\n",
        "val_accuracy = 100 * correct / total\n",
        "print(f'test Loss: {val_loss / len(test_loader):.4f}, Test Accuracy: {val_accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DN_ne4QQDDWw",
        "outputId": "3f243732-3704-4fec-e2c1-0a276eb030a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 57/57 [00:02<00:00, 26.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test Loss: 0.6917, Test Accuracy: 80.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4U4NlG8vGPzN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}